{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for training the network for SVHN dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=floatX=float32, device=gpu2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: GeForce GTX TITAN X (CNMeM is disabled, cuDNN Version is too old. Update to v5, was 4004.)\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS=floatX=float32, device=gpu2\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from __future__ import print_function\n",
    "\n",
    "from BC_layers import DenseLayer, Conv2DLayer\n",
    "from BC_utils import calculate_update\n",
    "from pylearn2.datasets.svhn import SVHN\n",
    "from pylearn2.utils import serial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original implementation use a script for pre-processing. I also found preprocessing script [here](https://github.com/lisa-lab/pylearn2/blob/master/pylearn2/scripts/papers/maxout/svhn_preprocessing.py), and it seems that it is from the codes for Maxout Network. So here in order to make sure I have the same preprocessing result, I use the same code in the link.\n",
    "\n",
    "Moreover, the code for downloading and splitting the data is from [here](https://raw.githubusercontent.com/lisa-lab/pylearn2/master/pylearn2/scripts/datasets/download_svhn.sh), also in the pylearn2 library. Although we can also use the code in the previous homeworks, here I use that code for consistency of the overall training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make logging functions\n",
    "import logging\n",
    "logger = logging.getLogger('')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler('experiment.log')\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd2/yluo/temp/pylearn2/pylearn2/datasets/svhn.py:61: UserWarning: Because path is not same as PYLEARN2_DATA_PATH be aware that data might have been modified or pre-processed.\n",
      "  warnings.warn(\"Because path is not same as PYLEARN2_DATA_PATH \"\n",
      "WARNING:root:The dataset is saved as float64, changing theano's floatX to the same dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: SVHN_LOCAL_PATH=/mnt/hdd2/yluo/data/SVHN\n",
      "GCN processing data from 0 to 5000\n",
      "GCN processing data from 5000 to 10000\n",
      "GCN processing data from 10000 to 15000\n",
      "GCN processing data from 15000 to 20000\n",
      "GCN processing data from 20000 to 25000\n",
      "GCN processing data from 25000 to 30000\n",
      "GCN processing data from 30000 to 35000\n",
      "GCN processing data from 35000 to 40000\n",
      "GCN processing data from 40000 to 45000\n",
      "GCN processing data from 45000 to 50000\n",
      "GCN processing data from 50000 to 55000\n",
      "GCN processing data from 55000 to 60000\n",
      "GCN processing data from 60000 to 65000\n",
      "GCN processing data from 65000 to 70000\n",
      "GCN processing data from 70000 to 75000\n",
      "GCN processing data from 75000 to 80000\n",
      "GCN processing data from 80000 to 85000\n",
      "GCN processing data from 85000 to 90000\n",
      "GCN processing data from 90000 to 95000\n",
      "GCN processing data from 95000 to 100000\n",
      "GCN processing data from 100000 to 105000\n",
      "GCN processing data from 105000 to 110000\n",
      "GCN processing data from 110000 to 115000\n",
      "GCN processing data from 115000 to 120000\n",
      "GCN processing data from 120000 to 125000\n",
      "GCN processing data from 125000 to 130000\n",
      "GCN processing data from 130000 to 135000\n",
      "GCN processing data from 135000 to 140000\n",
      "GCN processing data from 140000 to 145000\n",
      "GCN processing data from 145000 to 150000\n",
      "GCN processing data from 150000 to 155000\n",
      "GCN processing data from 155000 to 160000\n",
      "GCN processing data from 160000 to 165000\n",
      "GCN processing data from 165000 to 170000\n",
      "GCN processing data from 170000 to 175000\n",
      "GCN processing data from 175000 to 180000\n",
      "GCN processing data from 180000 to 185000\n",
      "GCN processing data from 185000 to 190000\n",
      "GCN processing data from 190000 to 195000\n",
      "GCN processing data from 195000 to 200000\n",
      "GCN processing data from 200000 to 205000\n",
      "GCN processing data from 205000 to 210000\n",
      "GCN processing data from 210000 to 215000\n",
      "GCN processing data from 215000 to 220000\n",
      "GCN processing data from 220000 to 225000\n",
      "GCN processing data from 225000 to 230000\n",
      "GCN processing data from 230000 to 235000\n",
      "GCN processing data from 235000 to 240000\n",
      "GCN processing data from 240000 to 245000\n",
      "GCN processing data from 245000 to 250000\n",
      "GCN processing data from 250000 to 255000\n",
      "GCN processing data from 255000 to 260000\n",
      "GCN processing data from 260000 to 265000\n",
      "GCN processing data from 265000 to 270000\n",
      "GCN processing data from 270000 to 275000\n",
      "GCN processing data from 275000 to 280000\n",
      "GCN processing data from 280000 to 285000\n",
      "GCN processing data from 285000 to 290000\n",
      "GCN processing data from 290000 to 295000\n",
      "GCN processing data from 295000 to 300000\n",
      "GCN processing data from 300000 to 305000\n",
      "GCN processing data from 305000 to 310000\n",
      "GCN processing data from 310000 to 315000\n",
      "GCN processing data from 315000 to 320000\n",
      "GCN processing data from 320000 to 325000\n",
      "GCN processing data from 325000 to 330000\n",
      "GCN processing data from 330000 to 335000\n",
      "GCN processing data from 335000 to 340000\n",
      "GCN processing data from 340000 to 345000\n",
      "GCN processing data from 345000 to 350000\n",
      "GCN processing data from 350000 to 355000\n",
      "GCN processing data from 355000 to 360000\n",
      "GCN processing data from 360000 to 365000\n",
      "GCN processing data from 365000 to 370000\n",
      "GCN processing data from 370000 to 375000\n",
      "GCN processing data from 375000 to 380000\n",
      "GCN processing data from 380000 to 385000\n",
      "GCN processing data from 385000 to 390000\n",
      "GCN processing data from 390000 to 395000\n",
      "GCN processing data from 395000 to 400000\n",
      "GCN processing data from 400000 to 405000\n",
      "GCN processing data from 405000 to 410000\n",
      "GCN processing data from 410000 to 415000\n",
      "GCN processing data from 415000 to 420000\n",
      "GCN processing data from 420000 to 425000\n",
      "GCN processing data from 425000 to 430000\n",
      "GCN processing data from 430000 to 435000\n",
      "GCN processing data from 435000 to 440000\n",
      "GCN processing data from 440000 to 445000\n",
      "GCN processing data from 445000 to 450000\n",
      "GCN processing data from 450000 to 455000\n",
      "GCN processing data from 455000 to 460000\n",
      "GCN processing data from 460000 to 465000\n",
      "GCN processing data from 465000 to 470000\n",
      "GCN processing data from 470000 to 475000\n",
      "GCN processing data from 475000 to 480000\n",
      "GCN processing data from 480000 to 485000\n",
      "GCN processing data from 485000 to 490000\n",
      "GCN processing data from 490000 to 495000\n",
      "GCN processing data from 495000 to 500000\n",
      "GCN processing data from 500000 to 505000\n",
      "GCN processing data from 505000 to 510000\n",
      "GCN processing data from 510000 to 515000\n",
      "GCN processing data from 515000 to 520000\n",
      "GCN processing data from 520000 to 525000\n",
      "GCN processing data from 525000 to 530000\n",
      "GCN processing data from 530000 to 535000\n",
      "GCN processing data from 535000 to 540000\n",
      "GCN processing data from 540000 to 545000\n",
      "GCN processing data from 545000 to 550000\n",
      "GCN processing data from 550000 to 555000\n",
      "GCN processing data from 555000 to 560000\n",
      "GCN processing data from 560000 to 565000\n",
      "GCN processing data from 565000 to 570000\n",
      "GCN processing data from 570000 to 575000\n",
      "GCN processing data from 575000 to 580000\n",
      "GCN processing data from 580000 to 585000\n",
      "GCN processing data from 585000 to 590000\n",
      "GCN processing data from 590000 to 595000\n",
      "GCN processing data from 595000 to 600000\n",
      "LCN processing data from 0 to 5000\n",
      "LCN processing data from 5000 to 10000\n",
      "LCN processing data from 10000 to 15000\n",
      "LCN processing data from 15000 to 20000\n",
      "LCN processing data from 20000 to 25000\n",
      "LCN processing data from 25000 to 30000\n",
      "LCN processing data from 30000 to 35000\n",
      "LCN processing data from 35000 to 40000\n",
      "LCN processing data from 40000 to 45000\n",
      "LCN processing data from 45000 to 50000\n",
      "LCN processing data from 50000 to 55000\n",
      "LCN processing data from 55000 to 60000\n",
      "LCN processing data from 60000 to 65000\n",
      "LCN processing data from 65000 to 70000\n",
      "LCN processing data from 70000 to 75000\n",
      "LCN processing data from 75000 to 80000\n",
      "LCN processing data from 80000 to 85000\n",
      "LCN processing data from 85000 to 90000\n",
      "LCN processing data from 90000 to 95000\n",
      "LCN processing data from 95000 to 100000\n",
      "LCN processing data from 100000 to 105000\n",
      "LCN processing data from 105000 to 110000\n",
      "LCN processing data from 110000 to 115000\n",
      "LCN processing data from 115000 to 120000\n",
      "LCN processing data from 120000 to 125000\n",
      "LCN processing data from 125000 to 130000\n",
      "LCN processing data from 130000 to 135000\n",
      "LCN processing data from 135000 to 140000\n",
      "LCN processing data from 140000 to 145000\n",
      "LCN processing data from 145000 to 150000\n",
      "LCN processing data from 150000 to 155000\n",
      "LCN processing data from 155000 to 160000\n",
      "LCN processing data from 160000 to 165000\n",
      "LCN processing data from 165000 to 170000\n",
      "LCN processing data from 170000 to 175000\n",
      "LCN processing data from 175000 to 180000\n",
      "LCN processing data from 180000 to 185000\n",
      "LCN processing data from 185000 to 190000\n",
      "LCN processing data from 190000 to 195000\n",
      "LCN processing data from 195000 to 200000\n",
      "LCN processing data from 200000 to 205000\n",
      "LCN processing data from 205000 to 210000\n",
      "LCN processing data from 210000 to 215000\n",
      "LCN processing data from 215000 to 220000\n",
      "LCN processing data from 220000 to 225000\n",
      "LCN processing data from 225000 to 230000\n",
      "LCN processing data from 230000 to 235000\n",
      "LCN processing data from 235000 to 240000\n",
      "LCN processing data from 240000 to 245000\n",
      "LCN processing data from 245000 to 250000\n",
      "LCN processing data from 250000 to 255000\n",
      "LCN processing data from 255000 to 260000\n",
      "LCN processing data from 260000 to 265000\n",
      "LCN processing data from 265000 to 270000\n",
      "LCN processing data from 270000 to 275000\n",
      "LCN processing data from 275000 to 280000\n",
      "LCN processing data from 280000 to 285000\n",
      "LCN processing data from 285000 to 290000\n",
      "LCN processing data from 290000 to 295000\n",
      "LCN processing data from 295000 to 300000\n",
      "LCN processing data from 300000 to 305000\n",
      "LCN processing data from 305000 to 310000\n",
      "LCN processing data from 310000 to 315000\n",
      "LCN processing data from 315000 to 320000\n",
      "LCN processing data from 320000 to 325000\n",
      "LCN processing data from 325000 to 330000\n",
      "LCN processing data from 330000 to 335000\n",
      "LCN processing data from 335000 to 340000\n",
      "LCN processing data from 340000 to 345000\n",
      "LCN processing data from 345000 to 350000\n",
      "LCN processing data from 350000 to 355000\n",
      "LCN processing data from 355000 to 360000\n",
      "LCN processing data from 360000 to 365000\n",
      "LCN processing data from 365000 to 370000\n",
      "LCN processing data from 370000 to 375000\n",
      "LCN processing data from 375000 to 380000\n",
      "LCN processing data from 380000 to 385000\n",
      "LCN processing data from 385000 to 390000\n",
      "LCN processing data from 390000 to 395000\n",
      "LCN processing data from 395000 to 400000\n",
      "LCN processing data from 400000 to 405000\n",
      "LCN processing data from 405000 to 410000\n",
      "LCN processing data from 410000 to 415000\n",
      "LCN processing data from 415000 to 420000\n",
      "LCN processing data from 420000 to 425000\n",
      "LCN processing data from 425000 to 430000\n",
      "LCN processing data from 430000 to 435000\n",
      "LCN processing data from 435000 to 440000\n",
      "LCN processing data from 440000 to 445000\n",
      "LCN processing data from 445000 to 450000\n",
      "LCN processing data from 450000 to 455000\n",
      "LCN processing data from 455000 to 460000\n",
      "LCN processing data from 460000 to 465000\n",
      "LCN processing data from 465000 to 470000\n",
      "LCN processing data from 470000 to 475000\n",
      "LCN processing data from 475000 to 480000\n",
      "LCN processing data from 480000 to 485000\n",
      "LCN processing data from 485000 to 490000\n",
      "LCN processing data from 490000 to 495000\n",
      "LCN processing data from 495000 to 500000\n",
      "LCN processing data from 500000 to 505000\n",
      "LCN processing data from 505000 to 510000\n",
      "LCN processing data from 510000 to 515000\n",
      "LCN processing data from 515000 to 520000\n",
      "LCN processing data from 520000 to 525000\n",
      "LCN processing data from 525000 to 530000\n",
      "LCN processing data from 530000 to 535000\n",
      "LCN processing data from 535000 to 540000\n",
      "LCN processing data from 540000 to 545000\n",
      "LCN processing data from 545000 to 550000\n",
      "LCN processing data from 550000 to 555000\n",
      "LCN processing data from 555000 to 560000\n",
      "LCN processing data from 560000 to 565000\n",
      "LCN processing data from 565000 to 570000\n",
      "LCN processing data from 570000 to 575000\n",
      "LCN processing data from 575000 to 580000\n",
      "LCN processing data from 580000 to 585000\n",
      "LCN processing data from 585000 to 590000\n",
      "LCN processing data from 590000 to 595000\n",
      "LCN processing data from 595000 to 598388\n",
      "GCN processing data from 0 to 5000\n",
      "GCN processing data from 5000 to 10000\n",
      "LCN processing data from 0 to 5000\n",
      "LCN processing data from 5000 to 6000\n",
      "GCN processing data from 0 to 5000\n",
      "GCN processing data from 5000 to 10000\n",
      "GCN processing data from 10000 to 15000\n",
      "GCN processing data from 15000 to 20000\n",
      "GCN processing data from 20000 to 25000\n",
      "GCN processing data from 25000 to 30000\n",
      "LCN processing data from 0 to 5000\n",
      "LCN processing data from 5000 to 10000\n",
      "LCN processing data from 10000 to 15000\n",
      "LCN processing data from 15000 to 20000\n",
      "LCN processing data from 20000 to 25000\n",
      "LCN processing data from 25000 to 26032\n"
     ]
    }
   ],
   "source": [
    "# make sure you have downloaded the data in the correct directory\n",
    "\n",
    "# make need to change the dir here on different machine\n",
    "%env SVHN_LOCAL_PATH=/mnt/hdd2/yluo/data/SVHN \n",
    "%run make_SVHN.py\n",
    "%run SVHN_preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, here I use the same code to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SVHN dataset\n",
      "env: SVHN_LOCAL_PATH=/mnt/hdd2/yluo/data/SVHN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd2/yluo/temp/pylearn2/pylearn2/datasets/svhn.py:61: UserWarning: Because path is not same as PYLEARN2_DATA_PATH be aware that data might have been modified or pre-processed.\n",
      "  warnings.warn(\"Because path is not same as PYLEARN2_DATA_PATH \"\n"
     ]
    }
   ],
   "source": [
    "print('Loading SVHN dataset')\n",
    "%env SVHN_LOCAL_PATH=/mnt/hdd2/yluo/data/SVHN   \n",
    "\n",
    "train_set = SVHN(\n",
    "    which_set= 'splitted_train',\n",
    "    path= \"${SVHN_LOCAL_PATH}\",\n",
    "    axes= ['b', 'c', 0, 1])\n",
    "     \n",
    "valid_set = SVHN(\n",
    "    which_set= 'valid',\n",
    "    path= \"${SVHN_LOCAL_PATH}\",\n",
    "    axes= ['b', 'c', 0, 1])\n",
    "    \n",
    "test_set = SVHN(\n",
    "    which_set= 'test',\n",
    "    path= \"${SVHN_LOCAL_PATH}\",\n",
    "    axes= ['b', 'c', 0, 1])\n",
    "    \n",
    "# bc01 format\n",
    "# print train_set.X.shape\n",
    "train_set.X = np.reshape(train_set.X,(-1,3,32,32)).astype('float32')\n",
    "valid_set.X = np.reshape(valid_set.X,(-1,3,32,32)).astype('float32')\n",
    "test_set.X = np.reshape(test_set.X,(-1,3,32,32)).astype('float32')\n",
    "    \n",
    "# for hinge loss \n",
    "# make targets onehot\n",
    "def make_onehot(dataset, category=10):\n",
    "    n_dataset = np.zeros((dataset.shape[0], category), dtype=np.int32)\n",
    "    n_dataset[np.arange(n_dataset.shape[0]), dataset[:,0]-1] = 1\n",
    "    return n_dataset\n",
    "\n",
    "train_set.y = np.subtract(np.multiply(2,make_onehot(train_set.y)),1.).astype('int32')\n",
    "valid_set.y = np.subtract(np.multiply(2,make_onehot(valid_set.y)),1.).astype('int32')\n",
    "test_set.y = np.subtract(np.multiply(2,make_onehot(test_set.y)),1.).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make dataset shared variables\n",
    "from theano import shared\n",
    "train_set_x = shared(train_set.X, borrow=True)\n",
    "train_set_y = shared(train_set.y, borrow=True)\n",
    "valid_set_x = shared(valid_set.X, borrow=True)\n",
    "valid_set_y = shared(valid_set.y, borrow=True)\n",
    "test_set_x = shared(test_set.X, borrow=True)\n",
    "test_set_y = shared(test_set.y, borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import batch_norm\n",
    "def make_network(input_shape, net_arch, net_spec):\n",
    "    assert len(net_arch) == len(net_spec)\n",
    "    \n",
    "    layer = lasagne.layers.InputLayer(shape=input_shape)\n",
    "    \n",
    "    layers = {'in': layer}\n",
    "\n",
    "    for i in range(len(net_arch)):\n",
    "        if net_arch[i]=='noise':\n",
    "            lasagne.layers.GaussianNoiseLayer(layer,**net_spec[i])\n",
    "        \n",
    "        elif net_arch[i]=='dropout':\n",
    "            layer = lasagne.layers.DropoutLayer(layer,**net_spec[i])\n",
    "        \n",
    "        elif net_arch[i]=='reshape':\n",
    "            layer = lasagne.layers.ReshapeLayer(layer,**net_spec[i])\n",
    "        \n",
    "        elif net_arch[i]=='cnn':\n",
    "            layer = Conv2DLayer(layer, **net_spec[i])\n",
    "        \n",
    "        elif net_arch[i]=='bn':\n",
    "            layer = lasagne.layers.batch_norm(layer, **net_spec[i])\n",
    "            \n",
    "        elif net_arch[i]=='maxpool':\n",
    "            layer = lasagne.layers.MaxPool2DLayer(layer, **net_spec[i])\n",
    "            \n",
    "        elif net_arch[i][:11]=='feedforward':\n",
    "            if net_arch[i]=='feedforward_tanh':\n",
    "                nonlinearity = lasagne.nonlinearities.tanh\n",
    "            elif net_arch[i]=='feedforward_sigmoid':\n",
    "                nonlinearity = lasagne.nonlinearities.sigmoid\n",
    "            elif net_arch[i]=='feedforward_softmax':\n",
    "                nonlinearity = lasagne.nonlinearities.softmax\n",
    "            elif net_arch[i]=='feedforward_linear':\n",
    "                nonlinearity = lasagne.nonlinearities.linear\n",
    "            elif net_arch[i]=='feedforward_rectify':\n",
    "                nonlinearity = lasagne.nonlinearities.rectify\n",
    "            elif net_arch[i]=='feedforward_leaky':\n",
    "                nonlinearity = lasagne.nonlinearities.leaky_rectify\n",
    "            elif net_arch[i]=='feedforward_identity':\n",
    "                nonlinearity = lasagne.nonlinearities.identity\n",
    "                \n",
    "            layer = DenseLayer(layer,nonlinearity=nonlinearity,**net_spec[i])\n",
    "                \n",
    "                \n",
    "    layers['out'] = layer\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make network\n",
    "binary=True\n",
    "stochastic=True\n",
    "\n",
    "net_arch=['cnn','bn','cnn','maxpool','bn','cnn','bn','cnn','maxpool','bn','cnn','bn','cnn','maxpool','bn',\n",
    "         'feedforward_rectify', 'bn', 'feedforward_rectify', 'bn', 'feedforward_identity', 'bn']\n",
    "\n",
    "net_spec=[{'binary':binary,'stochastic':stochastic,'num_filters':64,'filter_size':(3,3),'pad':1}, \n",
    "          {'name': 'batch norm 1'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_filters':64,'filter_size':(3,3),'pad':1}, \n",
    "          {'pool_size':(2, 2)}, \n",
    "          {'name': 'batch norm 2'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_filters':128,'filter_size':(3,3),'pad':1}, \n",
    "          {'name': 'batch norm 3'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_filters':128,'filter_size':(3,3),'pad':1}, \n",
    "          {'pool_size':(2, 2)}, \n",
    "          {'name': 'batch norm 4'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_filters':256,'filter_size':(3,3),'pad':1}, \n",
    "          {'name': 'batch norm 5'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_filters':256,'filter_size':(3,3),'pad':1}, \n",
    "          {'pool_size':(2, 2)}, \n",
    "          {'name': 'batch norm 6'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_units':1024}, \n",
    "          {'name': 'batch norm 7'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_units':1024}, \n",
    "          {'name': 'batch norm 8'}, \n",
    "          {'binary':binary,'stochastic':stochastic,'num_units':10}, \n",
    "          {'name': 'batch norm 9'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_shape=(None, 3, 32, 32)\n",
    "Layer = make_network(input_shape,net_arch,net_spec)\n",
    "\n",
    "input = T.ftensor4('input')\n",
    "Y = T.imatrix('Y')\n",
    "LR = T.fscalar('learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cost function\n",
    "# hinge loss\n",
    "cost_tr = T.mean(T.sqr(T.maximum(0.,1. - Y * lasagne.layers.get_output(Layer['out'], \n",
    "                                                                     {Layer['in']: input}, deterministic=False))))\n",
    "\n",
    "cost_cv = T.mean(T.sqr(T.maximum(0.,1. - Y * lasagne.layers.get_output(Layer['out'], \n",
    "                                                                     {Layer['in']: input}, deterministic=True))))\n",
    "error_cv = T.mean(T.neq(T.argmax(lasagne.layers.get_output(Layer['out'], \\\n",
    "                                                           {Layer['in']: input}, deterministic=True), axis=1), \n",
    "                        T.argmax(Y, axis=1)),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute update\n",
    "update = calculate_update(Layer, cost_tr, LR, binary=binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Computing function for training...\n",
      "Computing function for training...\n",
      "INFO:root:Compiling cost function for testing...\n",
      "Compiling cost function for testing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent:  8.615265131\n",
      "Time spent: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Compiling cost function for validation...\n",
      "Compiling cost function for validation...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9.26482701302\n",
      "Time spent:  9.91819596291\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "n_train_batches = train_set_x.get_value(borrow=True).shape[0]\n",
    "n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "n_train_batches //= batch_size\n",
    "n_valid_batches //= batch_size\n",
    "n_test_batches //= batch_size\n",
    "\n",
    "\n",
    "index = T.lscalar()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "logger.info('Computing function for training...')\n",
    "train_model = theano.function(\n",
    "        inputs=[index, LR],\n",
    "        outputs=cost_tr,\n",
    "        updates=update,\n",
    "        givens={\n",
    "            input: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            Y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "print('Time spent: ', time.time() - start_time)\n",
    "\n",
    "logger.info('Compiling cost function for testing...')\n",
    "test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=error_cv,\n",
    "        givens={\n",
    "            input: test_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            Y: test_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "print('Time spent: ', time.time() - start_time)\n",
    "\n",
    "logger.info('Compiling cost function for validation...')\n",
    "validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=error_cv,\n",
    "        givens={\n",
    "            input: valid_set_x[index * batch_size:(index + 1) * batch_size],\n",
    "            Y: valid_set_y[index * batch_size:(index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "print('Time spent: ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training function\n",
    "import timeit\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def train_nn(train_model, validate_model, test_model, LR, \n",
    "            n_train_batches, n_valid_batches, n_test_batches, n_epochs,\n",
    "            verbose = True):\n",
    "    \"\"\"\n",
    "    Function from HW3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 1e9  # look as this many examples regardless\n",
    "    patience_increase = 5  # wait this much longer when a new best is\n",
    "                           # found\n",
    "    improvement_threshold = 0.98  # a relative improvement of this much is\n",
    "                                   # considered significant\n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    \n",
    "    LR_decay = (0.0003/LR)**(1./n_epochs)\n",
    "\n",
    "    while epoch < n_epochs:\n",
    "        epoch = epoch + 1\n",
    "        start_epoch_time = time.time()\n",
    "        \n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            cost_ij = train_model(minibatch_index, LR)\n",
    "\n",
    "        validation_losses = [validate_model(i) for i\n",
    "                                     in range(n_valid_batches)]\n",
    "        this_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "        if verbose:\n",
    "            print('epoch %i, validation error %f %%' %\n",
    "                        (epoch,\n",
    "                         this_validation_loss * 100.))\n",
    "\n",
    "        # if we got the best validation score until now\n",
    "        if this_validation_loss < best_validation_loss:\n",
    "            # save best validation score and iteration number\n",
    "            best_validation_loss = this_validation_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "            # test it on the test set\n",
    "            test_losses = [\n",
    "                        test_model(i)\n",
    "                        for i in range(n_test_batches)\n",
    "                ]\n",
    "            test_score = np.mean(test_losses)\n",
    "\n",
    "            if verbose:\n",
    "                print(('     epoch %i, test error of '\n",
    "                               'best model %f %%') %\n",
    "                              (epoch, \n",
    "                               test_score * 100.))\n",
    "        \n",
    "        # decay learning rate\n",
    "        LR *= LR_decay\n",
    "        \n",
    "        logger.info(\"Epoch {} took {} seconds.\\n\".format(\n",
    "            epoch, time.time() - start_epoch_time))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    # Print out summary\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f %% obtained at epoch %i, '\n",
    "          'with test performance %f %%' %\n",
    "          (best_validation_loss * 100., best_epoch, test_score * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the network\n",
    "print('... training')\n",
    "\n",
    "train_nn(train_model, validate_model, test_model, 0.1,\n",
    "        n_train_batches, n_valid_batches, n_test_batches, 100, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
